{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3YkgE1gzzvDx"
      },
      "source": [
        "# **RETAIN heart failure detection**\n",
        "\n",
        "An implementation of [RETAIN: An Interpretable Predictive Model for Healthcare using Reverse Time Attention Mechanism](https://arxiv.org/abs/1608.05745) by Choi et al. to predict heart failure based on clinical records."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kUxhhon0Wpb1"
      },
      "source": [
        "### 1. Load libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 289,
      "metadata": {
        "id": "p3m2pkyXWe9a"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pickle as pickle\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zhlg8yPmaYu1"
      },
      "source": [
        "# **Prepare dataset:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t_WapQJYWzX3"
      },
      "source": [
        "### 2. Load dataset\n",
        "Dataset consists of several files:\n",
        "* **pids:** patient ID\n",
        "* **vids:** visit ID\n",
        "* **hfs:** heart failures\n",
        "* **seqs:** sequences of ICD-9 clinical records\n",
        "* **types:** mapping of ICD-9 code to ID\n",
        "* **rtypes:** index mapping of ICD-9 ID, used to simplify code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 290,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5gxJrwBlWKAH",
        "outputId": "c2519413-9439-48af-b5a7-3c29e921a23a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "pids = pd.read_pickle('/dataset/pids.pkl')\n",
        "vids = pd.read_pickle('/dataset/vids.pkl')\n",
        "hfs = pd.read_pickle('/dataset/hfs.pkl')\n",
        "seqs = pd.read_pickle('/dataset/seqs.pkl')\n",
        "types = pd.read_pickle('/dataset/types.pkl')\n",
        "rtypes = pd.read_pickle('/dataset/rtypes.pkl')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oiABRJ7aXF6d"
      },
      "source": [
        "### 3. Data preview"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 291,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "id": "c0rmNJeEXIFc",
        "outputId": "e3758802-e6aa-4cdc-8213-aae969f7efb6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total patients: 1000\n",
            "Total amount of patients with heart failure: 548\n",
            "Rato of heart failure patients: 0.548\n",
            "\n",
            "Example patient:\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-b7b316ea-a5ab-49a5-90d1-9c45467b22aa\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ID</th>\n",
              "      <th>Visits</th>\n",
              "      <th>Heart failure</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>47537</td>\n",
              "      <td>2</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b7b316ea-a5ab-49a5-90d1-9c45467b22aa')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-b7b316ea-a5ab-49a5-90d1-9c45467b22aa button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-b7b316ea-a5ab-49a5-90d1-9c45467b22aa');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "      ID  Visits  Heart failure\n",
              "3  47537       2          False"
            ]
          },
          "execution_count": 291,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def print_patient_data(index):\n",
        "    patient_stats = pd.DataFrame({\n",
        "        \"ID\": pids[index],\n",
        "        \"Visits\": len(vids[index]),\n",
        "        \"Heart failure\": bool(hfs[index])\n",
        "    }, index=[index])\n",
        "\n",
        "    return patient_stats\n",
        "\n",
        "\n",
        "print(\"Total patients:\", len(pids))\n",
        "print(\"Total amount of patients with heart failure:\", sum(hfs))\n",
        "print(\"Rato of heart failure patients:\", (sum(hfs) / len(hfs)))\n",
        "print(\"\\nExample patient:\")\n",
        "print_patient_data(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 292,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 538
        },
        "id": "u-RE51WiYkWp",
        "outputId": "e387ddc0-fe8d-46eb-c17c-a0e8730579a7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Details of patient visit:\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-40bcf4d9-95b9-4347-b99c-94cb7db70934\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>event_id</th>\n",
              "      <th>event</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>12</td>\n",
              "      <td>DIAG_041</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>103</td>\n",
              "      <td>DIAG_276</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>262</td>\n",
              "      <td>DIAG_518</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>285</td>\n",
              "      <td>DIAG_560</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>290</td>\n",
              "      <td>DIAG_567</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>292</td>\n",
              "      <td>DIAG_569</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>359</td>\n",
              "      <td>DIAG_707</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>416</td>\n",
              "      <td>DIAG_785</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>39</td>\n",
              "      <td>DIAG_155</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>225</td>\n",
              "      <td>DIAG_456</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>275</td>\n",
              "      <td>DIAG_537</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>294</td>\n",
              "      <td>DIAG_571</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>326</td>\n",
              "      <td>DIAG_608</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>267</td>\n",
              "      <td>DIAG_529</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>93</td>\n",
              "      <td>DIAG_263</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-40bcf4d9-95b9-4347-b99c-94cb7db70934')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-40bcf4d9-95b9-4347-b99c-94cb7db70934 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-40bcf4d9-95b9-4347-b99c-94cb7db70934');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "    event_id     event\n",
              "0         12  DIAG_041\n",
              "1        103  DIAG_276\n",
              "2        262  DIAG_518\n",
              "3        285  DIAG_560\n",
              "4        290  DIAG_567\n",
              "5        292  DIAG_569\n",
              "6        359  DIAG_707\n",
              "7        416  DIAG_785\n",
              "8         39  DIAG_155\n",
              "9        225  DIAG_456\n",
              "10       275  DIAG_537\n",
              "11       294  DIAG_571\n",
              "12       326  DIAG_608\n",
              "13       267  DIAG_529\n",
              "14        93  DIAG_263"
            ]
          },
          "execution_count": 292,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def get_patient_visit(index, visit):\n",
        "\n",
        "    patient_visit = pd.DataFrame({\n",
        "        'event_id': seqs[index][visit]\n",
        "    })\n",
        "\n",
        "    patient_visit['event'] = patient_visit.event_id.map(rtypes)\n",
        "\n",
        "    return patient_visit\n",
        "\n",
        "\n",
        "print(\"Details of patient visit:\")\n",
        "get_patient_visit(3, 0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I7p0RBXqY0cu"
      },
      "source": [
        "### 3. Prepare dataset - padding, masking, reversing:\n",
        "\n",
        "- **Padding:** inserts 0 to each visit to fill the gap between this visit and visit with highest length. We do that to have same dimensions across all sequences.\n",
        "- **Masking:** creates a boolean mask to indicate which element was added.\n",
        "- **Reversing:** according to paper we have to reverse the data to achieve \"reverse time attention mechanism\". We do reversing only on actual data not the padding."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 293,
      "metadata": {
        "id": "SzhAaSi8Y-uh"
      },
      "outputs": [],
      "source": [
        "def collate_fn(data):\n",
        "\n",
        "    sequences, labels = zip(*data)\n",
        "\n",
        "    num_patients = len(sequences)\n",
        "    max_num_visits = 0\n",
        "    max_num_codes = 0\n",
        "\n",
        "    for patient in sequences:\n",
        "        max_num_visits = max(max_num_visits, len(patient))\n",
        "        for visit in patient:\n",
        "            max_num_codes = max(max_num_codes, len(visit))\n",
        "\n",
        "    shape = (num_patients, max_num_visits, max_num_codes)\n",
        "\n",
        "    y = torch.tensor(labels, dtype=torch.float)\n",
        "    x = torch.zeros(shape, dtype=torch.long)\n",
        "    rev_x = torch.zeros(shape, dtype=torch.long)\n",
        "    masks = torch.zeros(shape, dtype=torch.bool)\n",
        "    rev_masks = torch.zeros(shape, dtype=torch.bool)\n",
        "\n",
        "    for i, patient in enumerate(sequences):\n",
        "        for j, visit in enumerate(patient):\n",
        "\n",
        "            x[i,j,:len(visit)] = torch.tensor(visit, dtype=torch.long)\n",
        "            masks[i,j, :len(visit)] = 1\n",
        "\n",
        "            rev_x[i, len(patient) - j - 1, :len(visit)] = torch.tensor(visit, dtype=torch.long)\n",
        "            rev_masks[i, len(patient) - j - 1, :len(visit)] = 1\n",
        "            \n",
        "    \n",
        "    return x, masks, rev_x, rev_masks, y\n",
        "\n",
        "\n",
        "def load_data(train_dataset, val_dataset, collate_fn):\n",
        "\n",
        "    batch_size = 32\n",
        "\n",
        "    train_loader = torch.utils.data.DataLoader(\n",
        "        train_dataset, \n",
        "        batch_size=batch_size, \n",
        "        shuffle=True, \n",
        "        collate_fn=collate_fn\n",
        "    )\n",
        "    \n",
        "    val_loader = torch.utils.data.DataLoader(\n",
        "        val_dataset, \n",
        "        batch_size=batch_size, \n",
        "        shuffle=True, \n",
        "        collate_fn=collate_fn\n",
        "    )\n",
        "    \n",
        "    return train_loader, val_loader\n",
        "\n",
        "\n",
        "\n",
        "class CustomDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, seqs, hfs):\n",
        "        self.x = seqs\n",
        "        self.y = hfs\n",
        "    \n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.x)\n",
        "    \n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return (self.x[index], self.y[index])\n",
        "\n",
        "\n",
        "dataset = CustomDataset(seqs, hfs)\n",
        "split = int(len(dataset)*0.8)\n",
        "\n",
        "lengths = [split, len(dataset) - split]\n",
        "train_dataset, val_dataset = torch.utils.data.dataset.random_split(dataset, lengths)\n",
        "\n",
        "\n",
        "train_loader, val_loader = load_data(train_dataset, val_dataset, collate_fn)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mff6VKOmaUSU"
      },
      "source": [
        "# **RETAIN model**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yN_kbNHnbQmc"
      },
      "source": [
        "### 4. Custom layers\n",
        "- Alpha attention\n",
        "- Beta attention\n",
        "- Attention sum\n",
        "- Sum embeddings with applied mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 294,
      "metadata": {
        "id": "Bvkxfr3Iaw5z"
      },
      "outputs": [],
      "source": [
        "class AlphaAttention(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, hidden_dim):\n",
        "        super().__init__()\n",
        "        self.a_att = nn.Linear(hidden_dim, 1)\n",
        "\n",
        "\n",
        "    def forward(self, g):\n",
        "        alpha = self.a_att(g)\n",
        "        alpha = torch.softmax(alpha, 1)\n",
        "        return alpha\n",
        "\n",
        "\n",
        "class BetaAttention(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, hidden_dim):\n",
        "        super().__init__()\n",
        "        self.b_att = nn.Linear(hidden_dim, hidden_dim)\n",
        "\n",
        "\n",
        "    def forward(self, h):\n",
        "        beta = self.b_att(h)\n",
        "        beta = torch.tanh(beta)\n",
        "        return beta\n",
        "\n",
        "\n",
        "def attention_sum(alpha, beta, rev_v, rev_masks):\n",
        "    c = (torch.sum(rev_masks, -1) > 0).type(torch.float)\n",
        "    c = c.unsqueeze(-1)\n",
        "    c = torch.sum(alpha * beta * rev_v * c, dim=1)\n",
        "    return c\n",
        "\n",
        "\n",
        "def sum_embeddings_with_mask(x, masks):\n",
        "    x = x * masks.unsqueeze(-1)\n",
        "    x = torch.sum(x, dim = -2)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uMggOiEEboO0"
      },
      "source": [
        "### 5. Model architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 295,
      "metadata": {
        "id": "0QAB2bSSbtpI"
      },
      "outputs": [],
      "source": [
        "class RETAIN(nn.Module):\n",
        "    \n",
        "    def __init__(self, num_codes, embedding_dim=128):\n",
        "        super().__init__()\n",
        "\n",
        "        self.embedding = nn.Embedding(num_codes, embedding_dim)\n",
        "\n",
        "        self.rnn_a = nn.GRU(embedding_dim, embedding_dim, batch_first=True)\n",
        "        self.rnn_b = nn.GRU(embedding_dim, embedding_dim, batch_first=True)\n",
        "\n",
        "        self.att_a = AlphaAttention(embedding_dim)\n",
        "        self.att_b = BetaAttention(embedding_dim)\n",
        "\n",
        "        self.fc = nn.Linear(embedding_dim, 1)\n",
        "\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "\n",
        "    def forward(self, x, masks, rev_x, rev_masks):\n",
        "\n",
        "        rev_x = self.embedding(rev_x)\n",
        "        rev_x = sum_embeddings_with_mask(rev_x, rev_masks)\n",
        "\n",
        "        g, _ = self.rnn_a(rev_x)\n",
        "        h, _ = self.rnn_b(rev_x)\n",
        "\n",
        "        alpha = self.att_a(g)\n",
        "        beta = self.att_b(h)\n",
        "\n",
        "        c = attention_sum(alpha, beta, rev_x, rev_masks)\n",
        "\n",
        "        logits = self.fc(c)\n",
        "        probs = self.sigmoid(logits)\n",
        "        return probs.squeeze()\n",
        "    \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bCedt11vcOnU"
      },
      "source": [
        "### 6. Evaluation function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 296,
      "metadata": {
        "id": "fkL6qAOycRrq"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import precision_recall_fscore_support, roc_auc_score\n",
        "\n",
        "\n",
        "def eval(model, val_loader):\n",
        "\n",
        "    \n",
        "    y_pred = torch.LongTensor()\n",
        "    y_score = torch.Tensor()\n",
        "    y_true = torch.LongTensor()\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    for x, masks, rev_x, rev_masks, y in val_loader:\n",
        "        y_logit = model(x, masks, rev_x, rev_masks)\n",
        "\n",
        "        y_hat = y_logit >= 0.5\n",
        "\n",
        "        y_score = torch.cat((y_score,  y_logit.detach().to('cpu')), dim=0)\n",
        "        y_pred = torch.cat((y_pred,  y_hat.detach().to('cpu')), dim=0)\n",
        "        y_true = torch.cat((y_true, y.detach().to('cpu')), dim=0)\n",
        "    \n",
        "    p, r, f, _ = precision_recall_fscore_support(y_true, y_pred, average='binary')\n",
        "    roc_auc = roc_auc_score(y_true, y_score)\n",
        "    \n",
        "    return p, r, f, roc_auc\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aEdfIsylcgKF"
      },
      "source": [
        "### 7. Train the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 297,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lRv0D3BwcjGg",
        "outputId": "ae4df602-9860-4209-cb43-c3b4e7b7dbb4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 0 \t Training Loss: 0.667214\n",
            "Epoch: 0 \t Validation p: 0.68, r:0.86, f: 0.76, roc_auc: 0.67\n",
            "Epoch: 10 \t Training Loss: 0.075948\n",
            "Epoch: 10 \t Validation p: 0.81, r:0.72, f: 0.76, roc_auc: 0.80\n",
            "Epoch: 20 \t Training Loss: 0.012533\n",
            "Epoch: 20 \t Validation p: 0.80, r:0.73, f: 0.77, roc_auc: 0.80\n",
            "Epoch: 30 \t Training Loss: 0.005566\n",
            "Epoch: 30 \t Validation p: 0.79, r:0.74, f: 0.77, roc_auc: 0.80\n",
            "Epoch: 40 \t Training Loss: 0.003681\n",
            "Epoch: 40 \t Validation p: 0.79, r:0.74, f: 0.76, roc_auc: 0.80\n",
            "Epoch: 50 \t Training Loss: 0.002958\n",
            "Epoch: 50 \t Validation p: 0.79, r:0.74, f: 0.76, roc_auc: 0.80\n",
            "Epoch: 60 \t Training Loss: 0.002613\n",
            "Epoch: 60 \t Validation p: 0.79, r:0.74, f: 0.76, roc_auc: 0.80\n",
            "Epoch: 70 \t Training Loss: 0.002289\n",
            "Epoch: 70 \t Validation p: 0.79, r:0.74, f: 0.76, roc_auc: 0.80\n",
            "Epoch: 80 \t Training Loss: 0.002281\n",
            "Epoch: 80 \t Validation p: 0.79, r:0.74, f: 0.76, roc_auc: 0.80\n",
            "Epoch: 90 \t Training Loss: 0.002101\n",
            "Epoch: 90 \t Validation p: 0.79, r:0.74, f: 0.76, roc_auc: 0.80\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "0.8"
            ]
          },
          "execution_count": 297,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "retain = RETAIN(num_codes = len(types))\n",
        "\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = torch.optim.Adam(retain.parameters(), lr=0.0003)\n",
        "n_epochs = 100\n",
        "\n",
        "\n",
        "def train(model, train_loader, val_loader, n_epochs):\n",
        "\n",
        "    model.train()\n",
        "    \n",
        "    for epoch in range(n_epochs):\n",
        "\n",
        "        train_loss = 0\n",
        "\n",
        "        for x, masks, rev_x, rev_masks, y in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            y_hat = model(x, masks, rev_x, rev_masks)\n",
        "            loss = criterion(y_hat, y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            train_loss += loss.item()\n",
        "\n",
        "\n",
        "        if epoch % 10 == 0:\n",
        "            train_loss = train_loss / len(train_loader)\n",
        "            print('Epoch: {} \\t Training Loss: {:.6f}'.format(epoch, train_loss))\n",
        "            p, r, f, roc_auc = eval(model, val_loader)\n",
        "            print('Epoch: {} \\t Validation p: {:.2f}, r:{:.2f}, f: {:.2f}, roc_auc: {:.2f}'.format(epoch, p, r, f, roc_auc))\n",
        "\n",
        "    return round(roc_auc, 2)\n",
        "\n",
        "\n",
        "train(retain, train_loader, val_loader, n_epochs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bCI7If71wajf"
      },
      "source": [
        "### 8. Evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 298,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "19YF4dZmyc7K",
        "outputId": "e3a55436-5c48-467b-b4d7-b241617eeb10"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(0.7876106194690266, 0.7416666666666667, 0.7639484978540773, 0.80125)\n",
            "RETAIN(\n",
            "  (embedding): Embedding(619, 128)\n",
            "  (rnn_a): GRU(128, 128, batch_first=True)\n",
            "  (rnn_b): GRU(128, 128, batch_first=True)\n",
            "  (att_a): AlphaAttention(\n",
            "    (a_att): Linear(in_features=128, out_features=1, bias=True)\n",
            "  )\n",
            "  (att_b): BetaAttention(\n",
            "    (b_att): Linear(in_features=128, out_features=128, bias=True)\n",
            "  )\n",
            "  (fc): Linear(in_features=128, out_features=1, bias=True)\n",
            "  (sigmoid): Sigmoid()\n",
            ")\n",
            "tensor([False,  True,  True,  True, False,  True,  True, False, False,  True,\n",
            "         True, False, False,  True,  True,  True,  True, False, False, False,\n",
            "        False, False, False, False,  True,  True, False,  True,  True, False,\n",
            "         True,  True])\n",
            "tensor([False,  True, False, False, False,  True,  True,  True,  True,  True,\n",
            "        False,  True, False, False, False, False,  True, False, False,  True,\n",
            "         True, False,  True,  True,  True,  True,  True,  True, False, False,\n",
            "         True,  True])\n",
            "tensor([False,  True,  True,  True, False, False, False,  True,  True,  True,\n",
            "         True,  True, False,  True, False, False,  True,  True, False,  True,\n",
            "         True,  True, False, False, False,  True, False, False, False,  True,\n",
            "         True,  True])\n",
            "tensor([False,  True, False,  True,  True, False, False,  True, False, False,\n",
            "         True,  True,  True,  True, False,  True,  True, False,  True, False,\n",
            "        False,  True,  True,  True, False, False,  True, False, False, False,\n",
            "         True,  True])\n",
            "tensor([ True, False, False,  True, False,  True, False, False,  True,  True,\n",
            "        False, False,  True,  True, False, False,  True, False,  True,  True,\n",
            "        False,  True,  True,  True,  True, False,  True, False,  True,  True,\n",
            "         True,  True])\n",
            "tensor([ True, False,  True,  True,  True, False, False, False, False, False,\n",
            "         True, False,  True, False,  True, False, False,  True, False,  True,\n",
            "         True,  True,  True, False, False,  True,  True,  True,  True,  True,\n",
            "        False, False])\n",
            "tensor([ True,  True,  True, False,  True,  True,  True,  True])\n"
          ]
        }
      ],
      "source": [
        "print(eval(retain, val_loader))\n",
        "print(retain)\n",
        "\n",
        "for x, masks, rev_x, rev_masks, y in val_loader:\n",
        "    y_hat = retain(x, masks, rev_x, rev_masks)\n",
        "    y_hat = y_hat >= 0.5\n",
        "    print(y_hat)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V_QyeyXV3LeT"
      },
      "source": [
        "### 9. Save the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 299,
      "metadata": {
        "id": "awQNO4Mawgk-"
      },
      "outputs": [],
      "source": [
        "torch.save(retain, \"model.pt\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.9.6 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.6"
    },
    "vscode": {
      "interpreter": {
        "hash": "11938c6bc6919ae2720b4d5011047913343b08a43b18698fd82dedb0d4417594"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
